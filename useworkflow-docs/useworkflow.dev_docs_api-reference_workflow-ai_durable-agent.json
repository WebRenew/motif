{
  "markdown": "[Vercel](https://vercel.com/) Slash [Workflow DevKit LogoWorkflow](https://useworkflow.dev/)\n\n- [Docs](https://useworkflow.dev/docs)\n- [Examples](https://github.com/vercel/workflow-examples)\n\n[X](https://x.com/workflowdevkit) [GitHub](https://github.com/vercel/workflow) Search... `⌘K`Ask AI\n\nAsk AI\n\nSearch... `⌘K`\n\n[Getting Started](https://useworkflow.dev/docs/getting-started)\n\n- [Next.js](https://useworkflow.dev/docs/getting-started/next)\n- [Vite](https://useworkflow.dev/docs/getting-started/vite)\n- [Astro](https://useworkflow.dev/docs/getting-started/astro)\n- [Express](https://useworkflow.dev/docs/getting-started/express)\n- [Fastify](https://useworkflow.dev/docs/getting-started/fastify)\n- [Hono](https://useworkflow.dev/docs/getting-started/hono)\n- [Nitro](https://useworkflow.dev/docs/getting-started/nitro)\n- [Nuxt](https://useworkflow.dev/docs/getting-started/nuxt)\n- [SvelteKit](https://useworkflow.dev/docs/getting-started/sveltekit)\n\n[Foundations](https://useworkflow.dev/docs/foundations)\n\n- [Workflows and Steps](https://useworkflow.dev/docs/foundations/workflows-and-steps)\n- [Starting Workflows](https://useworkflow.dev/docs/foundations/starting-workflows)\n- [Control Flow Patterns](https://useworkflow.dev/docs/foundations/control-flow-patterns)\n- [Errors & Retrying](https://useworkflow.dev/docs/foundations/errors-and-retries)\n- [Hooks & Webhooks](https://useworkflow.dev/docs/foundations/hooks)\n- [Streaming](https://useworkflow.dev/docs/foundations/streaming)\n- [Serialization](https://useworkflow.dev/docs/foundations/serialization)\n- [Idempotency](https://useworkflow.dev/docs/foundations/idempotency)\n\nHow it works\n\n[Observability](https://useworkflow.dev/docs/observability)\n\nAI Agents\n\n- [Building Durable AI Agents](https://useworkflow.dev/docs/ai)\n- [Streaming Updates from Tools](https://useworkflow.dev/docs/ai/streaming-updates-from-tools)\n- [Resumable Streams](https://useworkflow.dev/docs/ai/resumable-streams)\n- [Sleep, Suspense, and Scheduling](https://useworkflow.dev/docs/ai/sleep-and-delays)\n- [Human-in-the-Loop](https://useworkflow.dev/docs/ai/human-in-the-loop)\n- [Patterns for Defining Tools](https://useworkflow.dev/docs/ai/defining-tools)\n- [Chat Session Modeling](https://useworkflow.dev/docs/ai/chat-session-modeling)\n\n[Deploying](https://useworkflow.dev/docs/deploying)\n\n[Errors](https://useworkflow.dev/docs/errors)\n\n[API Reference](https://useworkflow.dev/docs/api-reference)\n\nDurableAgentAPI Signature\n\n[API Reference](https://useworkflow.dev/docs/api-reference) [@workflow/ai](https://useworkflow.dev/docs/api-reference/workflow-ai)\n\n# DurableAgent\n\nThe `@workflow/ai` package is currently in active development and should be considered experimental.\n\nThe `DurableAgent` class enables you to create AI-powered agents that can maintain state across workflow steps, call tools, and gracefully handle interruptions and resumptions.\n\nTool calls can be implemented as workflow steps for automatic retries, or as regular workflow-level logic utilizing core library features such as [`sleep()`](https://useworkflow.dev/docs/api-reference/workflow/sleep) and [Hooks](https://useworkflow.dev/docs/foundations/hooks).\n\n```\nimport { DurableAgent } from \"@workflow/ai/agent\";\nimport { getWritable } from \"workflow\";\nimport { z } from \"zod\";\nimport type { UIMessageChunk } from \"ai\";\n\nasync function getWeather({ city }: { city: string }) {\n  \"use step\";\n\n  return `Weather in ${city} is sunny`;\n}\n\nasync function myAgent() {\n  \"use workflow\";\n\n  const agent = new DurableAgent({\n    model: \"anthropic/claude-haiku-4.5\",\n    system: \"You are a helpful weather assistant.\",\n    temperature: 0.7,\n    tools: {\n      getWeather: {\n        description: \"Get weather for a city\",\n        inputSchema: z.object({ city: z.string() }),\n        execute: getWeather,\n      },\n    },\n  });\n\n  // The agent will stream its output to the workflow\n  // run's default output stream\n  const writable = getWritable<UIMessageChunk>();\n\n  const result = await agent.stream({\n    messages: [{ role: \"user\", content: \"How is the weather in San Francisco?\" }],\n    writable,\n  });\n\n  // result contains messages, steps, and optional structured output\n  console.log(result.messages);\n}\n```\n\n## [API Signature](https://useworkflow.dev/docs/api-reference/workflow-ai/durable-agent\\#api-signature)\n\n### [Class](https://useworkflow.dev/docs/api-reference/workflow-ai/durable-agent\\#class)\n\n| Name | Type | Description |\n| --- | --- | --- |\n| `model` | any |  |\n| `tools` | any |  |\n| `system` | any |  |\n| `generationSettings` | any |  |\n| `toolChoice` | any |  |\n| `telemetry` | any |  |\n| `generate` | () =\\> void |  |\n| `stream` | <TTools extends TBaseTools = TBaseTools, OUTPUT = never, PARTIAL\\_OUTPUT = never>(options: DurableAgentStreamOptions<TTools, OUTPUT, PARTIAL\\_OUTPUT>) => Promise<...> |  |\n\n### [DurableAgentOptions](https://useworkflow.dev/docs/api-reference/workflow-ai/durable-agent\\#durableagentoptions)\n\n| Name | Type | Description |\n| --- | --- | --- |\n| `model` | string \\| (() => Promise<CompatibleLanguageModel>) | The model provider to use for the agent.<br>This should be a string compatible with the Vercel AI Gateway (e.g., 'anthropic/claude-opus'),<br>or a step function that returns a LanguageModel instance (V2 or V3 depending on AI SDK version). |\n| `tools` | ToolSet | A set of tools available to the agent.<br>Tools can be implemented as workflow steps for automatic retries and persistence,<br>or as regular workflow-level logic using core library features like sleep() and Hooks. |\n| `system` | string | Optional system prompt to guide the agent's behavior. |\n| `toolChoice` | ToolChoice<ToolSet> | The tool choice strategy. Default: 'auto'. |\n| `experimental_telemetry` | TelemetrySettings | Optional telemetry configuration (experimental). |\n| `maxOutputTokens` | number | Maximum number of tokens to generate. |\n| `temperature` | number | Temperature setting. The range depends on the provider and model.<br>It is recommended to set either `temperature` or `topP`, but not both. |\n| `topP` | number | Nucleus sampling. This is a number between 0 and 1.<br>E.g. 0.1 would mean that only tokens with the top 10% probability mass are considered.<br>It is recommended to set either `temperature` or `topP`, but not both. |\n| `topK` | number | Only sample from the top K options for each subsequent token.<br>Used to remove \"long tail\" low probability responses.<br>Recommended for advanced use cases only. You usually only need to use temperature. |\n| `presencePenalty` | number | Presence penalty setting. It affects the likelihood of the model to<br>repeat information that is already in the prompt.<br>The presence penalty is a number between -1 (increase repetition)<br>and 1 (maximum penalty, decrease repetition). 0 means no penalty. |\n| `frequencyPenalty` | number | Frequency penalty setting. It affects the likelihood of the model<br>to repeatedly use the same words or phrases.<br>The frequency penalty is a number between -1 (increase repetition)<br>and 1 (maximum penalty, decrease repetition). 0 means no penalty. |\n| `stopSequences` | string\\[\\] | Stop sequences. If set, the model will stop generating text when one of the stop sequences is generated.<br>Providers may have limits on the number of stop sequences. |\n| `seed` | number | The seed (integer) to use for random sampling. If set and supported<br>by the model, calls will generate deterministic results. |\n| `maxRetries` | number | Maximum number of retries. Set to 0 to disable retries.<br>Note: In workflow context, retries are typically handled by the workflow step mechanism. |\n| `abortSignal` | AbortSignal | Abort signal for cancelling the operation. |\n| `headers` | Record<string, string \\| undefined> | Additional HTTP headers to be sent with the request.<br>Only applicable for HTTP-based providers. |\n| `providerOptions` | SharedV2ProviderOptions | Additional provider-specific options. They are passed through<br>to the provider from the AI SDK and enable provider-specific<br>functionality that can be fully encapsulated in the provider. |\n\n### [DurableAgentStreamOptions](https://useworkflow.dev/docs/api-reference/workflow-ai/durable-agent\\#durableagentstreamoptions)\n\n| Name | Type | Description |\n| --- | --- | --- |\n| `messages` | ModelMessage\\[\\] | The conversation messages to process. Should follow the AI SDK's ModelMessage format. |\n| `system` | string | Optional system prompt override. If provided, overrides the system prompt from the constructor. |\n| `writable` | WritableStream<UIMessageChunk> | The stream to which the agent writes message chunks. For example, use `getWritable<UIMessageChunk>()` to write to the workflow's default output stream. |\n| `preventClose` | boolean | If true, prevents the writable stream from being closed after streaming completes.<br>Defaults to false (stream will be closed). |\n| `sendStart` | boolean | If true, sends a 'start' chunk at the beginning of the stream.<br>Defaults to true. |\n| `sendFinish` | boolean | If true, sends a 'finish' chunk at the end of the stream.<br>Defaults to true. |\n| `stopWhen` | StopCondition<NoInfer<ToolSet>> \\| StopCondition<NoInfer<ToolSet>>\\[\\] | Condition for stopping the generation when there are tool results in the last step.<br>When the condition is an array, any of the conditions can be met to stop the generation. |\n| `maxSteps` | number | Maximum number of sequential LLM calls (steps), e.g. when you use tool calls.<br>A maximum number can be set to prevent infinite loops in the case of misconfigured tools.<br>By default, it's unlimited (the agent loops until completion). |\n| `toolChoice` | ToolChoice<TTools> | The tool choice strategy. Default: 'auto'.<br>Overrides the toolChoice from the constructor if provided. |\n| `activeTools` | NoInfer<keyof TTools>\\[\\] | Limits the tools that are available for the model to call without<br>changing the tool call and result types in the result. |\n| `experimental_telemetry` | TelemetrySettings | Optional telemetry configuration (experimental). |\n| `experimental_context` | unknown | Context that is passed into tool execution.<br>Experimental (can break in patch releases). |\n| `experimental_output` | OutputSpecification<OUTPUT, PARTIAL\\_OUTPUT> | Optional specification for parsing structured outputs from the LLM response.<br>Use `Output.object({ schema })` for structured output or `Output.text()` for text output. |\n| `includeRawChunks` | boolean | Whether to include raw chunks from the provider in the stream.<br>When enabled, you will receive raw chunks with type 'raw' that contain the unprocessed data from the provider.<br>This allows access to cutting-edge provider features not yet wrapped by the AI SDK.<br>Defaults to false. |\n| `experimental_repairToolCall` | ToolCallRepairFunction<TTools> | A function that attempts to repair a tool call that failed to parse. |\n| `experimental_transform` | StreamTextTransform<TTools> \\| StreamTextTransform<TTools>\\[\\] | Optional stream transformations.<br>They are applied in the order they are provided.<br>The stream transformations must maintain the stream structure for streamText to work correctly. |\n| `experimental_download` | DownloadFunction | Custom download function to use for URLs.<br>By default, files are downloaded if the model does not support the URL for the given media type. |\n| `onStepFinish` | StreamTextOnStepFinishCallback<TTools> | Callback function to be called after each step completes. |\n| `onError` | StreamTextOnErrorCallback | Callback that is invoked when an error occurs during streaming.<br>You can use it to log errors. |\n| `onFinish` | StreamTextOnFinishCallback<TTools, OUTPUT> | Callback that is called when the LLM response and all request tool executions<br>(for tools that have an `execute` function) are finished. |\n| `onAbort` | StreamTextOnAbortCallback<TTools> | Callback that is called when the operation is aborted. |\n| `prepareStep` | PrepareStepCallback<TTools> | Callback function called before each step in the agent loop.<br>Use this to modify settings, manage context, or inject messages dynamically. |\n| `collectUIMessages` | boolean | If true, accumulates UIMessage\\[\\] during streaming.<br>The accumulated messages will be available in the `uiMessages` property of the result.<br>This is useful when you need the final UIMessage representation after streaming completes,<br>without having to re-read the stream. |\n| `maxOutputTokens` | number | Maximum number of tokens to generate. |\n| `temperature` | number | Temperature setting. The range depends on the provider and model.<br>It is recommended to set either `temperature` or `topP`, but not both. |\n| `topP` | number | Nucleus sampling. This is a number between 0 and 1.<br>E.g. 0.1 would mean that only tokens with the top 10% probability mass are considered.<br>It is recommended to set either `temperature` or `topP`, but not both. |\n| `topK` | number | Only sample from the top K options for each subsequent token.<br>Used to remove \"long tail\" low probability responses.<br>Recommended for advanced use cases only. You usually only need to use temperature. |\n| `presencePenalty` | number | Presence penalty setting. It affects the likelihood of the model to<br>repeat information that is already in the prompt.<br>The presence penalty is a number between -1 (increase repetition)<br>and 1 (maximum penalty, decrease repetition). 0 means no penalty. |\n| `frequencyPenalty` | number | Frequency penalty setting. It affects the likelihood of the model<br>to repeatedly use the same words or phrases.<br>The frequency penalty is a number between -1 (increase repetition)<br>and 1 (maximum penalty, decrease repetition). 0 means no penalty. |\n| `stopSequences` | string\\[\\] | Stop sequences. If set, the model will stop generating text when one of the stop sequences is generated.<br>Providers may have limits on the number of stop sequences. |\n| `seed` | number | The seed (integer) to use for random sampling. If set and supported<br>by the model, calls will generate deterministic results. |\n| `maxRetries` | number | Maximum number of retries. Set to 0 to disable retries.<br>Note: In workflow context, retries are typically handled by the workflow step mechanism. |\n| `abortSignal` | AbortSignal | Abort signal for cancelling the operation. |\n| `headers` | Record<string, string \\| undefined> | Additional HTTP headers to be sent with the request.<br>Only applicable for HTTP-based providers. |\n| `providerOptions` | SharedV2ProviderOptions | Additional provider-specific options. They are passed through<br>to the provider from the AI SDK and enable provider-specific<br>functionality that can be fully encapsulated in the provider. |\n\n### [DurableAgentStreamResult](https://useworkflow.dev/docs/api-reference/workflow-ai/durable-agent\\#durableagentstreamresult)\n\nThe result returned from the `stream()` method:\n\n| Name | Type | Description |\n| --- | --- | --- |\n| `messages` | ModelMessage\\[\\] | The final messages including all tool calls and results. |\n| `steps` | StepResult<TTools>\\[\\] | Details for all steps. |\n| `experimental_output` | OUTPUT | The generated structured output. It uses the `experimental_output` specification.<br>Only available when `experimental_output` is specified. |\n| `uiMessages` | UIMessage<unknown, UIDataTypes, UITools>\\[\\] | The accumulated UI messages from the stream.<br>Only available when `collectUIMessages` is set to `true` in the stream options. |\n\n### [GenerationSettings](https://useworkflow.dev/docs/api-reference/workflow-ai/durable-agent\\#generationsettings)\n\nSettings that control model generation behavior. These can be set on the constructor or overridden per-stream call:\n\n| Name | Type | Description |\n| --- | --- | --- |\n| `maxOutputTokens` | number | Maximum number of tokens to generate. |\n| `temperature` | number | Temperature setting. The range depends on the provider and model.<br>It is recommended to set either `temperature` or `topP`, but not both. |\n| `topP` | number | Nucleus sampling. This is a number between 0 and 1.<br>E.g. 0.1 would mean that only tokens with the top 10% probability mass are considered.<br>It is recommended to set either `temperature` or `topP`, but not both. |\n| `topK` | number | Only sample from the top K options for each subsequent token.<br>Used to remove \"long tail\" low probability responses.<br>Recommended for advanced use cases only. You usually only need to use temperature. |\n| `presencePenalty` | number | Presence penalty setting. It affects the likelihood of the model to<br>repeat information that is already in the prompt.<br>The presence penalty is a number between -1 (increase repetition)<br>and 1 (maximum penalty, decrease repetition). 0 means no penalty. |\n| `frequencyPenalty` | number | Frequency penalty setting. It affects the likelihood of the model<br>to repeatedly use the same words or phrases.<br>The frequency penalty is a number between -1 (increase repetition)<br>and 1 (maximum penalty, decrease repetition). 0 means no penalty. |\n| `stopSequences` | string\\[\\] | Stop sequences. If set, the model will stop generating text when one of the stop sequences is generated.<br>Providers may have limits on the number of stop sequences. |\n| `seed` | number | The seed (integer) to use for random sampling. If set and supported<br>by the model, calls will generate deterministic results. |\n| `maxRetries` | number | Maximum number of retries. Set to 0 to disable retries.<br>Note: In workflow context, retries are typically handled by the workflow step mechanism. |\n| `abortSignal` | AbortSignal | Abort signal for cancelling the operation. |\n| `headers` | Record<string, string \\| undefined> | Additional HTTP headers to be sent with the request.<br>Only applicable for HTTP-based providers. |\n| `providerOptions` | SharedV2ProviderOptions | Additional provider-specific options. They are passed through<br>to the provider from the AI SDK and enable provider-specific<br>functionality that can be fully encapsulated in the provider. |\n\n### [PrepareStepInfo](https://useworkflow.dev/docs/api-reference/workflow-ai/durable-agent\\#preparestepinfo)\n\nInformation passed to the `prepareStep` callback:\n\n| Name | Type | Description |\n| --- | --- | --- |\n| `model` | string \\| (() => Promise<CompatibleLanguageModel>) | The current model configuration (string or function).<br>The function should return a LanguageModel instance (V2 or V3 depending on AI SDK version). |\n| `stepNumber` | number | The current step number (0-indexed). |\n| `steps` | StepResult<TTools>\\[\\] | All previous steps with their results. |\n| `messages` | LanguageModelV2Prompt | The messages that will be sent to the model.<br>This is the LanguageModelV2Prompt format used internally. |\n| `experimental_context` | unknown | The context passed via the experimental\\_context setting (experimental). |\n\n### [PrepareStepResult](https://useworkflow.dev/docs/api-reference/workflow-ai/durable-agent\\#preparestepresult)\n\nReturn type from the `prepareStep` callback:\n\n| Name | Type | Description |\n| --- | --- | --- |\n| `model` | string \\| (() => Promise<CompatibleLanguageModel>) | Override the model for this step.<br>The function should return a LanguageModel instance (V2 or V3 depending on AI SDK version). |\n| `system` | string | Override the system message for this step. |\n| `messages` | LanguageModelV2Prompt | Override the messages for this step.<br>Use this for context management or message injection. |\n| `toolChoice` | ToolChoice<ToolSet> | Override the tool choice for this step. |\n| `activeTools` | string\\[\\] | Override the active tools for this step.<br>Limits the tools that are available for the model to call. |\n| `experimental_context` | unknown | Context that is passed into tool execution. Experimental.<br>Changing the context will affect the context in this step and all subsequent steps. |\n| `maxOutputTokens` | number | Maximum number of tokens to generate. |\n| `temperature` | number | Temperature setting. The range depends on the provider and model.<br>It is recommended to set either `temperature` or `topP`, but not both. |\n| `topP` | number | Nucleus sampling. This is a number between 0 and 1.<br>E.g. 0.1 would mean that only tokens with the top 10% probability mass are considered.<br>It is recommended to set either `temperature` or `topP`, but not both. |\n| `topK` | number | Only sample from the top K options for each subsequent token.<br>Used to remove \"long tail\" low probability responses.<br>Recommended for advanced use cases only. You usually only need to use temperature. |\n| `presencePenalty` | number | Presence penalty setting. It affects the likelihood of the model to<br>repeat information that is already in the prompt.<br>The presence penalty is a number between -1 (increase repetition)<br>and 1 (maximum penalty, decrease repetition). 0 means no penalty. |\n| `frequencyPenalty` | number | Frequency penalty setting. It affects the likelihood of the model<br>to repeatedly use the same words or phrases.<br>The frequency penalty is a number between -1 (increase repetition)<br>and 1 (maximum penalty, decrease repetition). 0 means no penalty. |\n| `stopSequences` | string\\[\\] | Stop sequences. If set, the model will stop generating text when one of the stop sequences is generated.<br>Providers may have limits on the number of stop sequences. |\n| `seed` | number | The seed (integer) to use for random sampling. If set and supported<br>by the model, calls will generate deterministic results. |\n| `maxRetries` | number | Maximum number of retries. Set to 0 to disable retries.<br>Note: In workflow context, retries are typically handled by the workflow step mechanism. |\n| `abortSignal` | AbortSignal | Abort signal for cancelling the operation. |\n| `headers` | Record<string, string \\| undefined> | Additional HTTP headers to be sent with the request.<br>Only applicable for HTTP-based providers. |\n| `providerOptions` | SharedV2ProviderOptions | Additional provider-specific options. They are passed through<br>to the provider from the AI SDK and enable provider-specific<br>functionality that can be fully encapsulated in the provider. |\n\n### [TelemetrySettings](https://useworkflow.dev/docs/api-reference/workflow-ai/durable-agent\\#telemetrysettings)\n\nConfiguration for observability and telemetry:\n\n| Name | Type | Description |\n| --- | --- | --- |\n| `isEnabled` | boolean | Enable or disable telemetry. Defaults to true. |\n| `functionId` | string | Identifier for this function. Used to group telemetry data by function. |\n| `metadata` | Record<string, string \\| number \\| boolean \\| (string \\| number \\| boolean)\\[\\] \\| null \\| undefined> | Additional information to include in the telemetry data. |\n| `tracer` | unknown | Custom tracer for the telemetry. |\n\n### [Callbacks](https://useworkflow.dev/docs/api-reference/workflow-ai/durable-agent\\#callbacks)\n\n#### [StreamTextOnFinishCallback](https://useworkflow.dev/docs/api-reference/workflow-ai/durable-agent\\#streamtextonfinishcallback)\n\nCalled when streaming completes:\n\n| Name | Type | Description |\n| --- | --- | --- |\n| `event` | { readonly steps: StepResult<TTools>\\[\\]; readonly messages: ModelMessage\\[\\]; readonly experimental\\_context: unknown; readonly experimental\\_output: OUTPUT; } |  |\n\n`void | PromiseLike<void>`\n\n#### [StreamTextOnErrorCallback](https://useworkflow.dev/docs/api-reference/workflow-ai/durable-agent\\#streamtextonerrorcallback)\n\nCalled when an error occurs:\n\n| Name | Type | Description |\n| --- | --- | --- |\n| `event` | { error: unknown; } |  |\n\n`void | PromiseLike<void>`\n\n#### [StreamTextOnAbortCallback](https://useworkflow.dev/docs/api-reference/workflow-ai/durable-agent\\#streamtextonabortcallback)\n\nCalled when the operation is aborted:\n\n| Name | Type | Description |\n| --- | --- | --- |\n| `event` | { readonly steps: StepResult<TTools>\\[\\]; } |  |\n\n`void | PromiseLike<void>`\n\n### [Advanced Types](https://useworkflow.dev/docs/api-reference/workflow-ai/durable-agent\\#advanced-types)\n\n#### [ToolCallRepairFunction](https://useworkflow.dev/docs/api-reference/workflow-ai/durable-agent\\#toolcallrepairfunction)\n\nFunction to repair malformed tool calls:\n\n| Name | Type | Description |\n| --- | --- | --- |\n| `options` | { toolCall: LanguageModelV2ToolCall; tools: TTools; error: unknown; messages: LanguageModelV2Prompt; } |  |\n\n`LanguageModelV2ToolCall | Promise<LanguageModelV2ToolCall | null> | null`\n\n#### [StreamTextTransform](https://useworkflow.dev/docs/api-reference/workflow-ai/durable-agent\\#streamtexttransform)\n\nTransform applied to the stream:\n\n| Name | Type | Description |\n| --- | --- | --- |\n| `options` | { tools: TTools; stopStream: () => void; } |  |\n\n`TransformStream<LanguageModelV2StreamPart, LanguageModelV2StreamPart>`\n\n#### [OutputSpecification](https://useworkflow.dev/docs/api-reference/workflow-ai/durable-agent\\#outputspecification)\n\nSpecification for structured output parsing:\n\n| Name | Type | Description |\n| --- | --- | --- |\n| `type` | \"object\" \\| \"text\" |  |\n| `responseFormat` | { type: \"text\"; } \\| { type: \"json\"; schema?: JSONSchema7; name?: string; description?: string; } \\| undefined |  |\n| `parsePartial` | (options: { text: string; }) => Promise<{ partial: PARTIAL; } \\| undefined> |  |\n| `parseOutput` | (options: { text: string; }, context: { response: LanguageModelResponseMetadata; usage: LanguageModelV2Usage; finishReason: LanguageModelV2FinishReason; }) => Promise<...> |  |\n\n## [Key Features](https://useworkflow.dev/docs/api-reference/workflow-ai/durable-agent\\#key-features)\n\n- **Durable Execution**: Agents can be interrupted and resumed without losing state\n- **Flexible Tool Implementation**: Tools can be implemented as workflow steps for automatic retries, or as regular workflow-level logic\n- **Stream Processing**: Handles streaming responses and tool calls in a structured way\n- **Workflow Native**: Fully integrated with Workflow DevKit for production-grade reliability\n- **AI SDK Parity**: Supports the same options as AI SDK's `streamText` including generation settings, callbacks, and structured output\n\n## [Good to Know](https://useworkflow.dev/docs/api-reference/workflow-ai/durable-agent\\#good-to-know)\n\n- Tools can be implemented as workflow steps (using `\"use step\"` for automatic retries), or as regular workflow-level logic\n- Tools can use core library features like `sleep()` and Hooks within their `execute` functions\n- The agent processes tool calls iteratively until completion or `maxSteps` is reached\n- **Default `maxSteps` is unlimited** \\- set a value to limit the number of LLM calls\n- The `stream()` method returns `{ messages, steps, experimental_output, uiMessages }` containing the full conversation history, step details, optional structured output, and optionally accumulated UI messages\n- Use `collectUIMessages: true` to accumulate `UIMessage[]` during streaming, useful for persisting conversation state without re-reading the stream\n- The `prepareStep` callback runs before each step and can modify model, messages, generation settings, tool choice, and context\n- Generation settings (temperature, maxOutputTokens, etc.) can be set on the constructor and overridden per-stream call\n- Use `activeTools` to limit which tools are available for a specific stream call\n- The `onFinish` callback is called when all steps complete; `onAbort` is called if aborted\n\n## [Examples](https://useworkflow.dev/docs/api-reference/workflow-ai/durable-agent\\#examples)\n\n### [Basic Agent with Tools](https://useworkflow.dev/docs/api-reference/workflow-ai/durable-agent\\#basic-agent-with-tools)\n\n```\nimport { DurableAgent } from \"@workflow/ai/agent\";\nimport { getWritable } from \"workflow\";\nimport { z } from \"zod\";\nimport type { UIMessageChunk } from \"ai\";\n\nasync function getWeather({ location }: { location: string }) {\n  \"use step\";\n  // Fetch weather data\n  const response = await fetch(`https://api.weather.com?location=${location}`);\n  return response.json();\n}\n\nasync function weatherAgentWorkflow(userQuery: string) {\n  \"use workflow\";\n\n  const agent = new DurableAgent({\n    model: \"anthropic/claude-haiku-4.5\",\n    tools: {\n      getWeather: {\n        description: \"Get current weather for a location\",\n        inputSchema: z.object({ location: z.string() }),\n        execute: getWeather,\n      },\n    },\n    system: \"You are a helpful weather assistant. Always provide accurate weather information.\",\n  });\n\n  await agent.stream({\n    messages: [\\\n      {\\\n        role: \"user\",\\\n        content: userQuery,\\\n      },\\\n    ],\n    writable: getWritable<UIMessageChunk>(),\n  });\n}\n```\n\n### [Multiple Tools](https://useworkflow.dev/docs/api-reference/workflow-ai/durable-agent\\#multiple-tools)\n\n```\nimport { DurableAgent } from \"@workflow/ai/agent\";\nimport { getWritable } from \"workflow\";\nimport { z } from \"zod\";\nimport type { UIMessageChunk } from \"ai\";\n\nasync function getWeather({ location }: { location: string }) {\n  \"use step\";\n  return `Weather in ${location}: Sunny, 72°F`;\n}\n\nasync function searchEvents({ location, category }: { location: string; category: string }) {\n  \"use step\";\n  return `Found 5 ${category} events in ${location}`;\n}\n\nasync function multiToolAgentWorkflow(userQuery: string) {\n  \"use workflow\";\n\n  const agent = new DurableAgent({\n    model: \"anthropic/claude-haiku-4.5\",\n    tools: {\n      getWeather: {\n        description: \"Get weather for a location\",\n        inputSchema: z.object({ location: z.string() }),\n        execute: getWeather,\n      },\n      searchEvents: {\n        description: \"Search for upcoming events in a location\",\n        inputSchema: z.object({ location: z.string(), category: z.string() }),\n        execute: searchEvents,\n      },\n    },\n  });\n\n  await agent.stream({\n    messages: [\\\n      {\\\n        role: \"user\",\\\n        content: userQuery,\\\n      },\\\n    ],\n    writable: getWritable<UIMessageChunk>(),\n  });\n}\n```\n\n### [Multi-turn Conversation](https://useworkflow.dev/docs/api-reference/workflow-ai/durable-agent\\#multi-turn-conversation)\n\n```\nimport { DurableAgent } from \"@workflow/ai/agent\";\nimport { z } from \"zod\";\n\nasync function searchProducts({ query }: { query: string }) {\n  \"use step\";\n  // Search product database\n  return `Found 3 products matching \"${query}\"`;\n}\n\nasync function multiTurnAgentWorkflow() {\n  \"use workflow\";\n\n  const agent = new DurableAgent({\n    model: \"anthropic/claude-haiku-4.5\",\n    tools: {\n      searchProducts: {\n        description: \"Search for products\",\n        inputSchema: z.object({ query: z.string() }),\n        execute: searchProducts,\n      },\n    },\n  });\n\n  const writable = getWritable<UIMessageChunk>();\n\n  // First user message\n  //   - Result is streamed to the provided `writable` stream\n  //   - Message history is returned in `messages` for LLM context\n  let { messages } = await agent.stream({\n    messages: [\\\n      { role: \"user\", content: \"Find me some laptops\" }\\\n    ],\n    writable,\n  });\n\n  // Continue the conversation with the accumulated message history\n  const result = await agent.stream({\n    messages: [\\\n      ...messages,\\\n      { role: \"user\", content: \"Which one has the best battery life?\" }\\\n    ],\n    writable,\n  });\n\n  // result.messages now contains the complete conversation history\n  return result.messages;\n}\n```\n\n### [Tools with Workflow Library Features](https://useworkflow.dev/docs/api-reference/workflow-ai/durable-agent\\#tools-with-workflow-library-features)\n\n```\nimport { DurableAgent } from \"@workflow/ai/agent\";\nimport { sleep, defineHook, getWritable } from \"workflow\";\nimport { z } from \"zod\";\nimport type { UIMessageChunk } from \"ai\";\n\n// Define a reusable hook type\nconst approvalHook = defineHook<{ approved: boolean; reason: string }>();\n\nasync function scheduleTask({ delaySeconds }: { delaySeconds: number }) {\n  // Note: No \"use step\" for this tool call,\n  // since `sleep()` is a workflow level function\n  await sleep(`${delaySeconds}s`);\n  return `Slept for ${delaySeconds} seconds`;\n}\n\nasync function requestApproval({ message }: { message: string }) {\n  // Note: No \"use step\" for this tool call either,\n  // since hooks are awaited at the workflow level\n\n  // Utilize a Hook for Human-in-the-loop approval\n  const hook = approvalHook.create({\n    metadata: { message }\n  });\n\n  console.log(`Approval needed - token: ${hook.token}`);\n\n  // Wait for the approval payload\n  const approval = await hook;\n\n  if (approval.approved) {\n    return `Request approved: ${approval.reason}`;\n  } else {\n    throw new Error(`Request denied: ${approval.reason}`);\n  }\n}\n\nasync function agentWithLibraryFeaturesWorkflow(userRequest: string) {\n  \"use workflow\";\n\n  const agent = new DurableAgent({\n    model: \"anthropic/claude-haiku-4.5\",\n    tools: {\n      scheduleTask: {\n        description: \"Pause the workflow for the specified number of seconds\",\n        inputSchema: z.object({\n          delaySeconds: z.number(),\n        }),\n        execute: scheduleTask,\n      },\n      requestApproval: {\n        description: \"Request approval for an action\",\n        inputSchema: z.object({ message: z.string() }),\n        execute: requestApproval,\n      },\n    },\n  });\n\n  await agent.stream({\n    messages: [{ role: \"user\", content: userRequest }],\n    writable: getWritable<UIMessageChunk>(),\n  });\n}\n```\n\n### [Dynamic Context with prepareStep](https://useworkflow.dev/docs/api-reference/workflow-ai/durable-agent\\#dynamic-context-with-preparestep)\n\nUse `prepareStep` to modify settings before each step in the agent loop:\n\n```\nimport { DurableAgent } from \"@workflow/ai/agent\";\nimport { getWritable } from \"workflow\";\nimport type { UIMessageChunk } from \"ai\";\n\nasync function agentWithPrepareStep(userMessage: string) {\n  \"use workflow\";\n\n  const agent = new DurableAgent({\n    model: \"openai/gpt-4.1-mini\", // Default model\n    system: \"You are a helpful assistant.\",\n  });\n\n  await agent.stream({\n    messages: [{ role: \"user\", content: userMessage }],\n    writable: getWritable<UIMessageChunk>(),\n    prepareStep: async ({ stepNumber, messages }) => {\n      // Switch to a stronger model for complex reasoning after initial steps\n      if (stepNumber > 2 && messages.length > 10) {\n        return {\n          model: \"anthropic/claude-sonnet-4.5\",\n        };\n      }\n\n      // Trim context if messages grow too large\n      if (messages.length > 20) {\n        return {\n          messages: [\\\n            messages[0], // Keep system message\\\n            ...messages.slice(-10), // Keep last 10 messages\\\n          ],\n        };\n      }\n\n      return {}; // No changes\n    },\n  });\n}\n```\n\n### [Message Injection with prepareStep](https://useworkflow.dev/docs/api-reference/workflow-ai/durable-agent\\#message-injection-with-preparestep)\n\nInject messages from external sources (like hooks) before each LLM call:\n\n```\nimport { DurableAgent } from \"@workflow/ai/agent\";\nimport { getWritable, defineHook } from \"workflow\";\nimport type { UIMessageChunk } from \"ai\";\n\nconst messageHook = defineHook<{ message: string }>();\n\nasync function agentWithMessageQueue(initialMessage: string) {\n  \"use workflow\";\n\n  const messageQueue: Array<{ role: \"user\"; content: string }> = [];\n\n  // Listen for incoming messages via hook\n  const hook = messageHook.create();\n  hook.then(({ message }) => {\n    messageQueue.push({ role: \"user\", content: message });\n  });\n\n  const agent = new DurableAgent({\n    model: \"anthropic/claude-haiku-4.5\",\n    system: \"You are a helpful assistant.\",\n  });\n\n  await agent.stream({\n    messages: [{ role: \"user\", content: initialMessage }],\n    writable: getWritable<UIMessageChunk>(),\n    prepareStep: ({ messages }) => {\n      // Inject queued messages before the next step\n      if (messageQueue.length > 0) {\n        const newMessages = messageQueue.splice(0);\n        return {\n          messages: [\\\n            ...messages,\\\n            ...newMessages.map(m => ({\\\n              role: m.role,\\\n              content: [{ type: \"text\" as const, text: m.content }],\\\n            })),\\\n          ],\n        };\n      }\n      return {};\n    },\n  });\n}\n```\n\n### [Generation Settings](https://useworkflow.dev/docs/api-reference/workflow-ai/durable-agent\\#generation-settings)\n\nConfigure model generation parameters at the constructor or stream level:\n\n```\nimport { DurableAgent } from \"@workflow/ai/agent\";\nimport { getWritable } from \"workflow\";\nimport type { UIMessageChunk } from \"ai\";\n\nasync function agentWithGenerationSettings() {\n  \"use workflow\";\n\n  // Set default generation settings in constructor\n  const agent = new DurableAgent({\n    model: \"anthropic/claude-haiku-4.5\",\n    temperature: 0.7,\n    maxOutputTokens: 2000,\n    topP: 0.9,\n  });\n\n  // Override settings per-stream call\n  await agent.stream({\n    messages: [{ role: \"user\", content: \"Write a creative story\" }],\n    writable: getWritable<UIMessageChunk>(),\n    temperature: 0.9, // More creative for this call\n    maxSteps: 1,\n  });\n\n  // Use different settings for a different task\n  await agent.stream({\n    messages: [{ role: \"user\", content: \"Summarize this document precisely\" }],\n    writable: getWritable<UIMessageChunk>(),\n    temperature: 0.1, // More deterministic\n    maxSteps: 1,\n  });\n}\n```\n\n### [Limiting Steps with maxSteps](https://useworkflow.dev/docs/api-reference/workflow-ai/durable-agent\\#limiting-steps-with-maxsteps)\n\nBy default, the agent loops until completion. Use `maxSteps` to limit the number of LLM calls:\n\n```\nimport { DurableAgent } from \"@workflow/ai/agent\";\nimport { getWritable } from \"workflow\";\nimport { z } from \"zod\";\nimport type { UIMessageChunk } from \"ai\";\n\nasync function searchWeb({ query }: { query: string }) {\n  \"use step\";\n  return `Results for \"${query}\": ...`;\n}\n\nasync function analyzeResults({ data }: { data: string }) {\n  \"use step\";\n  return `Analysis: ${data}`;\n}\n\nasync function multiStepAgent() {\n  \"use workflow\";\n\n  const agent = new DurableAgent({\n    model: \"anthropic/claude-haiku-4.5\",\n    tools: {\n      searchWeb: {\n        description: \"Search the web for information\",\n        inputSchema: z.object({ query: z.string() }),\n        execute: searchWeb,\n      },\n      analyzeResults: {\n        description: \"Analyze search results\",\n        inputSchema: z.object({ data: z.string() }),\n        execute: analyzeResults,\n      },\n    },\n  });\n\n  // Limit to 10 steps for safety on complex research tasks\n  const result = await agent.stream({\n    messages: [{ role: \"user\", content: \"Research the latest AI trends and provide an analysis\" }],\n    writable: getWritable<UIMessageChunk>(),\n    maxSteps: 10,\n  });\n\n  // Access step-by-step details\n  console.log(`Completed in ${result.steps.length} steps`);\n}\n```\n\n### [Callbacks for Monitoring](https://useworkflow.dev/docs/api-reference/workflow-ai/durable-agent\\#callbacks-for-monitoring)\n\nUse callbacks to monitor streaming progress, handle errors, and react to completion:\n\n```\nimport { DurableAgent } from \"@workflow/ai/agent\";\nimport { getWritable } from \"workflow\";\nimport type { UIMessageChunk } from \"ai\";\n\nasync function agentWithCallbacks() {\n  \"use workflow\";\n\n  const agent = new DurableAgent({\n    model: \"anthropic/claude-haiku-4.5\",\n  });\n\n  await agent.stream({\n    messages: [{ role: \"user\", content: \"Hello!\" }],\n    writable: getWritable<UIMessageChunk>(),\n    maxSteps: 5,\n\n    // Called after each step completes\n    onStepFinish: async (step) => {\n      console.log(`Step finished: ${step.finishReason}`);\n      console.log(`Tokens used: ${step.usage.totalTokens}`);\n    },\n\n    // Called when streaming completes\n    onFinish: async ({ steps, messages }) => {\n      console.log(`Completed with ${steps.length} steps`);\n      console.log(`Final message count: ${messages.length}`);\n    },\n\n    // Called on errors\n    onError: async ({ error }) => {\n      console.error(\"Stream error:\", error);\n    },\n  });\n}\n```\n\n### [Structured Output](https://useworkflow.dev/docs/api-reference/workflow-ai/durable-agent\\#structured-output)\n\nParse structured data from the LLM response using `Output.object`:\n\n```\nimport { DurableAgent, Output } from \"@workflow/ai/agent\";\nimport { getWritable } from \"workflow\";\nimport { z } from \"zod\";\nimport type { UIMessageChunk } from \"ai\";\n\nasync function agentWithStructuredOutput() {\n  \"use workflow\";\n\n  const agent = new DurableAgent({\n    model: \"anthropic/claude-haiku-4.5\",\n  });\n\n  const result = await agent.stream({\n    messages: [{ role: \"user\", content: \"Analyze the sentiment of: 'I love this product!'\" }],\n    writable: getWritable<UIMessageChunk>(),\n    experimental_output: Output.object({\n      schema: z.object({\n        sentiment: z.enum([\"positive\", \"negative\", \"neutral\"]),\n        confidence: z.number().min(0).max(1),\n        reasoning: z.string(),\n      }),\n    }),\n  });\n\n  // Access the parsed structured output\n  console.log(result.experimental_output);\n  // { sentiment: \"positive\", confidence: 0.95, reasoning: \"...\" }\n}\n```\n\n### [Tool Choice Control](https://useworkflow.dev/docs/api-reference/workflow-ai/durable-agent\\#tool-choice-control)\n\nControl when and which tools the model can use:\n\n```\nimport { DurableAgent } from \"@workflow/ai/agent\";\nimport { getWritable } from \"workflow\";\nimport { z } from \"zod\";\nimport type { UIMessageChunk } from \"ai\";\n\nasync function agentWithToolChoice() {\n  \"use workflow\";\n\n  const agent = new DurableAgent({\n    model: \"anthropic/claude-haiku-4.5\",\n    tools: {\n      calculator: {\n        description: \"Perform calculations\",\n        inputSchema: z.object({ expression: z.string() }),\n        execute: async ({ expression }) => `Calculated: ${expression}`,\n      },\n      search: {\n        description: \"Search for information\",\n        inputSchema: z.object({ query: z.string() }),\n        execute: async ({ query }) => `Results for: ${query}`,\n      },\n    },\n    toolChoice: \"auto\", // Default: model decides\n  });\n\n  // Force the model to use a tool\n  await agent.stream({\n    messages: [{ role: \"user\", content: \"What is 2 + 2?\" }],\n    writable: getWritable<UIMessageChunk>(),\n    toolChoice: \"required\",\n    maxSteps: 2,\n  });\n\n  // Prevent tool usage\n  await agent.stream({\n    messages: [{ role: \"user\", content: \"Just chat with me\" }],\n    writable: getWritable<UIMessageChunk>(),\n    toolChoice: \"none\",\n  });\n\n  // Force a specific tool\n  await agent.stream({\n    messages: [{ role: \"user\", content: \"Calculate something\" }],\n    writable: getWritable<UIMessageChunk>(),\n    toolChoice: { type: \"tool\", toolName: \"calculator\" },\n    maxSteps: 2,\n  });\n\n  // Limit available tools for this call\n  await agent.stream({\n    messages: [{ role: \"user\", content: \"Just search, don't calculate\" }],\n    writable: getWritable<UIMessageChunk>(),\n    activeTools: [\"search\"],\n    maxSteps: 2,\n  });\n}\n```\n\n### [Passing Context to Tools](https://useworkflow.dev/docs/api-reference/workflow-ai/durable-agent\\#passing-context-to-tools)\n\nUse `experimental_context` to pass shared context to tool executions:\n\n```\nimport { DurableAgent } from \"@workflow/ai/agent\";\nimport { getWritable } from \"workflow\";\nimport { z } from \"zod\";\nimport type { UIMessageChunk } from \"ai\";\n\ninterface UserContext {\n  userId: string;\n  permissions: string[];\n}\n\nasync function agentWithContext(userId: string) {\n  \"use workflow\";\n\n  const agent = new DurableAgent({\n    model: \"anthropic/claude-haiku-4.5\",\n    tools: {\n      getUserData: {\n        description: \"Get user data\",\n        inputSchema: z.object({}),\n        execute: async (_, { experimental_context }) => {\n          const ctx = experimental_context as UserContext;\n          return { userId: ctx.userId, permissions: ctx.permissions };\n        },\n      },\n    },\n  });\n\n  await agent.stream({\n    messages: [{ role: \"user\", content: \"What are my permissions?\" }],\n    writable: getWritable<UIMessageChunk>(),\n    maxSteps: 2,\n    experimental_context: {\n      userId,\n      permissions: [\"read\", \"write\"],\n    } as UserContext,\n  });\n}\n```\n\n### [Collecting UI Messages](https://useworkflow.dev/docs/api-reference/workflow-ai/durable-agent\\#collecting-ui-messages)\n\nUse `collectUIMessages` to accumulate `UIMessage[]` during streaming. This is useful when you need to persist the conversation without re-reading the run's output stream:\n\n```\nimport { DurableAgent } from \"@workflow/ai/agent\";\nimport { getWritable } from \"workflow\";\nimport type { UIMessage, UIMessageChunk } from \"ai\";\n\nasync function agentWithUIMessages(userMessage: string) {\n  \"use workflow\";\n\n  const agent = new DurableAgent({\n    model: \"anthropic/claude-haiku-4.5\",\n    system: \"You are a helpful assistant.\",\n  });\n\n  const result = await agent.stream({\n    messages: [{ role: \"user\", content: userMessage }],\n    writable: getWritable<UIMessageChunk>(),\n    collectUIMessages: true,\n  });\n\n  // Access the accumulated UI messages\n  const uiMessages: UIMessage[] = result.uiMessages ?? [];\n\n  // Persist messages to a database\n  await saveConversation(uiMessages);\n\n  return result;\n}\n\nasync function saveConversation(messages: UIMessage[]) {\n  \"use step\";\n  // Save to database...\n}\n```\n\nThe `uiMessages` property is only available when `collectUIMessages` is set to `true`. When disabled, `uiMessages` is `undefined`.\n\n## [See Also](https://useworkflow.dev/docs/api-reference/workflow-ai/durable-agent\\#see-also)\n\n- [Building Durable AI Agents](https://useworkflow.dev/docs/ai) \\- Complete guide to creating durable agents\n- [Queueing User Messages](https://useworkflow.dev/docs/ai/message-queueing) \\- Using prepareStep for message injection\n- [WorkflowChatTransport](https://useworkflow.dev/docs/api-reference/workflow-ai/workflow-chat-transport) \\- Transport layer for AI SDK streams\n- [Workflows and Steps](https://useworkflow.dev/docs/foundations/workflows-and-steps) \\- Understanding workflow fundamentals\n- [AI SDK Loop Control](https://ai-sdk.dev/docs/agents/loop-control) \\- AI SDK's agent loop control patterns\n\n[@workflow/ai\\\\\n\\\\\nPrevious Page](https://useworkflow.dev/docs/api-reference/workflow-ai) [WorkflowChatTransport\\\\\n\\\\\nNext Page](https://useworkflow.dev/docs/api-reference/workflow-ai/workflow-chat-transport)\n\nOn this page\n\n[API Signature](https://useworkflow.dev/docs/api-reference/workflow-ai/durable-agent#api-signature) [Class](https://useworkflow.dev/docs/api-reference/workflow-ai/durable-agent#class) [DurableAgentOptions](https://useworkflow.dev/docs/api-reference/workflow-ai/durable-agent#durableagentoptions) [DurableAgentStreamOptions](https://useworkflow.dev/docs/api-reference/workflow-ai/durable-agent#durableagentstreamoptions) [DurableAgentStreamResult](https://useworkflow.dev/docs/api-reference/workflow-ai/durable-agent#durableagentstreamresult) [GenerationSettings](https://useworkflow.dev/docs/api-reference/workflow-ai/durable-agent#generationsettings) [PrepareStepInfo](https://useworkflow.dev/docs/api-reference/workflow-ai/durable-agent#preparestepinfo) [PrepareStepResult](https://useworkflow.dev/docs/api-reference/workflow-ai/durable-agent#preparestepresult) [TelemetrySettings](https://useworkflow.dev/docs/api-reference/workflow-ai/durable-agent#telemetrysettings) [Callbacks](https://useworkflow.dev/docs/api-reference/workflow-ai/durable-agent#callbacks) [StreamTextOnFinishCallback](https://useworkflow.dev/docs/api-reference/workflow-ai/durable-agent#streamtextonfinishcallback) [StreamTextOnErrorCallback](https://useworkflow.dev/docs/api-reference/workflow-ai/durable-agent#streamtextonerrorcallback) [StreamTextOnAbortCallback](https://useworkflow.dev/docs/api-reference/workflow-ai/durable-agent#streamtextonabortcallback) [Advanced Types](https://useworkflow.dev/docs/api-reference/workflow-ai/durable-agent#advanced-types) [ToolCallRepairFunction](https://useworkflow.dev/docs/api-reference/workflow-ai/durable-agent#toolcallrepairfunction) [StreamTextTransform](https://useworkflow.dev/docs/api-reference/workflow-ai/durable-agent#streamtexttransform) [OutputSpecification](https://useworkflow.dev/docs/api-reference/workflow-ai/durable-agent#outputspecification) [Key Features](https://useworkflow.dev/docs/api-reference/workflow-ai/durable-agent#key-features) [Good to Know](https://useworkflow.dev/docs/api-reference/workflow-ai/durable-agent#good-to-know) [Examples](https://useworkflow.dev/docs/api-reference/workflow-ai/durable-agent#examples) [Basic Agent with Tools](https://useworkflow.dev/docs/api-reference/workflow-ai/durable-agent#basic-agent-with-tools) [Multiple Tools](https://useworkflow.dev/docs/api-reference/workflow-ai/durable-agent#multiple-tools) [Multi-turn Conversation](https://useworkflow.dev/docs/api-reference/workflow-ai/durable-agent#multi-turn-conversation) [Tools with Workflow Library Features](https://useworkflow.dev/docs/api-reference/workflow-ai/durable-agent#tools-with-workflow-library-features) [Dynamic Context with prepareStep](https://useworkflow.dev/docs/api-reference/workflow-ai/durable-agent#dynamic-context-with-preparestep) [Message Injection with prepareStep](https://useworkflow.dev/docs/api-reference/workflow-ai/durable-agent#message-injection-with-preparestep) [Generation Settings](https://useworkflow.dev/docs/api-reference/workflow-ai/durable-agent#generation-settings) [Limiting Steps with maxSteps](https://useworkflow.dev/docs/api-reference/workflow-ai/durable-agent#limiting-steps-with-maxsteps) [Callbacks for Monitoring](https://useworkflow.dev/docs/api-reference/workflow-ai/durable-agent#callbacks-for-monitoring) [Structured Output](https://useworkflow.dev/docs/api-reference/workflow-ai/durable-agent#structured-output) [Tool Choice Control](https://useworkflow.dev/docs/api-reference/workflow-ai/durable-agent#tool-choice-control) [Passing Context to Tools](https://useworkflow.dev/docs/api-reference/workflow-ai/durable-agent#passing-context-to-tools) [Collecting UI Messages](https://useworkflow.dev/docs/api-reference/workflow-ai/durable-agent#collecting-ui-messages) [See Also](https://useworkflow.dev/docs/api-reference/workflow-ai/durable-agent#see-also)\n\n[GitHubEdit this page on GitHub](https://github.com/vercel/workflow/edit/main/content/docs/api-reference/workflow-ai/durable-agent.mdx) Scroll to topGive feedbackCopy pageAsk AI about this pageOpen in chat\n\n## Chat\n\nWhat is Workflow?How does retrying work?What control flow patterns are there?How do directives work?How do I build an AI agent?\n\nTip: You can open and close chat with `⌘I`\n\n0 / 1000",
  "metadata": {
    "twitter:image:type": "image/png",
    "title": "DurableAgent",
    "og:image:width": "1200",
    "ogImage": "https://useworkflow.dev/opengraph-image.png?opengraph-image.f38670ff.png",
    "twitter:image:height": "628",
    "twitter:title": "DurableAgent",
    "ogTitle": "DurableAgent",
    "viewport": "width=device-width, initial-scale=1",
    "og:image:height": "628",
    "twitter:card": "summary_large_image",
    "twitter:image:width": "1200",
    "twitter:image": "https://useworkflow.dev/opengraph-image.png?opengraph-image.f38670ff.png",
    "og:image:type": "image/png",
    "next-size-adjust": "",
    "language": "en",
    "og:title": "DurableAgent",
    "og:image": "https://useworkflow.dev/opengraph-image.png?opengraph-image.f38670ff.png",
    "favicon": "https://useworkflow.dev/favicon.ico?favicon.e7ce0d1c.ico",
    "scrapeId": "019bf2d4-569c-75d7-940f-a474371caad4",
    "sourceURL": "https://useworkflow.dev/docs/api-reference/workflow-ai/durable-agent",
    "url": "https://useworkflow.dev/docs/api-reference/workflow-ai/durable-agent",
    "statusCode": 200,
    "contentType": "text/html; charset=utf-8",
    "proxyUsed": "basic",
    "cacheState": "hit",
    "cachedAt": "2026-01-24T22:01:05.727Z",
    "creditsUsed": 1
  }
}